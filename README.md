# Scalable Data Pipeline with Snowflake, Apache Airflow, dbt, and Snowpark

A modern, cloud-native data pipeline that integrates **Snowflake**, **Apache Airflow**, **dbt**, and **Snowpark** to automate data ingestion, transformation, and advanced analytics — all orchestrated and containerized for scalability and reusability.


---

## Overview

This project showcases how to build an end-to-end pipeline for booking data using cutting-edge data engineering tools. From ingestion to insight, the pipeline is designed for automation, scalability, and cloud readiness. It simulates a real-world ETL system that handles transformations in SQL (via dbt) and Python (via Snowpark), all orchestrated with Apache Airflow.

---

## Tech Stack

- **Snowflake** – Scalable cloud data warehouse
- **Apache Airflow** – Workflow orchestration and scheduling
- **dbt** – SQL transformations and modeling
- **Snowpark** – Python-based data processing inside Snowflake
- **Cosmos** – Seamless dbt integration with Airflow
- **Docker** – Containerized local development

---

## Pipeline Workflow

### 1. Data Ingestion & Storage
- Load raw CSV files (`bookings_1`, `bookings_2`, `customers`) into Snowflake tables.

### 2. Transformations via dbt
- Clean and join raw data into a `prepped_data` model.
- Create analytic views:
  - `hotel_count_by_day` – Daily hotel bookings count
  - `30_day_avg_cost` – Rolling average of booking costs

### 3. Orchestration via Airflow
- Use Airflow DAGs to automate and monitor pipeline execution.
- Trigger `dbt` runs and custom Snowpark logic using Cosmos integration.
- Define task dependencies and scheduling for end-to-end automation.

### 4. Analytics with Snowpark
- Perform Python-based analysis inside Snowflake to:
  - Find the hotel with the highest booking cost
  - Enable complex data processing without moving data outside the warehouse

---

## Common Challenges and How They Were Solved

| Challenge                                  | Solution                                                                 |
|-------------------------------------------|--------------------------------------------------------------------------|
| Airflow-dbt integration issues             | Updated Cosmos config and standardized folder structure                  |
| Constraint violations in SQL models       | Added autogenerated surrogate keys to resolve unique constraint errors   |
| Snowflake connection errors in dbt        | Tuned `profiles.yml` to support private key auth and corrected schema    |
| Outdated references in documentation      | Rewrote configs to match latest dbt and Airflow best practices           |

---

## Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/SaiRanjithReddyK/Cloud-ETL-with-Airflow-and-Snowflake.git
cd Cloud-ETL-with-Airflow-and-Snowflake
```
### 2. Install Dependencies
```bash
pip install -r requirements.txt
```
### 3.Set Up Airflow
```bash
export AIRFLOW_HOME=$(pwd)/airflow
airflow db init
airflow scheduler & airflow webserver
```
### 4. Configure dbt Connection
--Update the profiles.yml file with your Snowflake credentials (account, schema, role, warehouse).
### 5.  Run the Pipeline
-Trigger the DAG
```bash
airflow dags trigger dbt_pipeline
```
## WHAT I GOT FROM THIS..?
--Created a fully automated, containerized ETL workflow using top cloud-native tools.
--Applied SQL transformations with dbt and in-warehouse Python analytics via Snowpark.
--Automated and scheduled using Apache Airflow with Cosmos integration.



